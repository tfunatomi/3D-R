{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3126f707-c0c3-4ec1-b7ed-bbc0ec186eb9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3D Reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a389f090-2005-481a-940f-5807cbd4f2cf",
   "metadata": {},
   "source": [
    "## Directory\n",
    "Download the data from NeuroGT (https://ssbd.riken.jp/neurogt/) and create the following directory before execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef07afd-9daa-4de0-a73a-d873d676cc1b",
   "metadata": {},
   "source": [
    "```\n",
    ".\n",
    "└── 3D-R/\n",
    "    ├── 3D-R.ipynb\n",
    "    └── dataset/\n",
    "        ├── 1\n",
    "        ├── 2\n",
    "        ├── 3\n",
    "        ├── ...\n",
    "        └── 84/\n",
    "            └── G2AN_woTM_L/\n",
    "                └── G2AN_woTM_xxxx_Lxxx_x.png\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17c9251-adf7-4c44-9881-ab498a4f0ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "#Creating Directories\n",
    "def mkdir(path):\n",
    "    if not os.path.exists(path):#ディレクトリがなかったら\n",
    "        os.mkdir(path)#作成したいフォルダ名を作成\n",
    "        \n",
    "        \n",
    "# 1 if all imaging datasets are download from the internet,\n",
    "# 0 if some imaging datasets have already been download locally, \n",
    "#    for example: \n",
    "#        dataset/\n",
    "#           1/\n",
    "#              G2AN_TM11.5_G/\n",
    "#.          2/\n",
    "#              G2AN_TM11.5_L/\n",
    "DOWNLOAD_ALL = 0\n",
    "\n",
    "if DOWNLOAD_ALL == 0:\n",
    "    txt = 'name.txt'\n",
    "    with open(txt) as f:\n",
    "        l_strip = [s.rstrip() for s in f.readlines()]\n",
    "        #print(len(l_strip), l_strip)\n",
    "    \n",
    "    n=0\n",
    "    for i in range(67,69):\n",
    "        dataset = 'dataset/'\n",
    "        mkdir(dataset)\n",
    "        \n",
    "        new = dataset+str(i)+'/'\n",
    "        mkdir(new)\n",
    "        \n",
    "        if i%2 == 0:\n",
    "            url='https://ssbd.riken.jp/neurogt/zip/'+l_strip[i//2-1]+str('L')+'.zip'\n",
    "            save_name='dataset/'+str(i)+'/'+l_strip[i//2-1]+str('L')+'/'+l_strip[i//2-1]+str('L')+'.zip'\n",
    "            n=n+1\n",
    "        \n",
    "        else:\n",
    "            url='https://ssbd.riken.jp/neurogt/zip/'+l_strip[i//2]+str('G')+'.zip'\n",
    "            save_name='dataset/'+str(i)+'/'+l_strip[i//2]+str('G')+'/'+l_strip[i//2]+str('G')+'.zip'\n",
    "    \n",
    "        urllib.request.urlretrieve(url, save_name)\n",
    "        print(url, save_name)\n",
    "        \n",
    "            \n",
    "elif DOWNLOAD_ALL == 1:\n",
    "    txt = 'name.txt'\n",
    "    with open(txt) as f:\n",
    "        l_strip = [s.rstrip() for s in f.readlines()]\n",
    "        print(l_strip)\n",
    "    \n",
    "    n=0\n",
    "    for i in range(1,85):\n",
    "        dataset = 'dataset/'\n",
    "        mkdir(dataset)\n",
    "        \n",
    "        new = dataset+str(i)+'/'\n",
    "        mkdir(new)\n",
    "        \n",
    "        \"\"\"\n",
    "        if i%2 == 0:\n",
    "            url='https://ssbd.riken.jp/neurogt/zip/'+l_strip[n]+str('L')+'.zip'\n",
    "            save_name='dataset/'+str(i)+'/'+l_strip[n]+str('L')+'/'+l_strip[n]+str('L')+'.zip'\n",
    "            n=n+1\n",
    "        \n",
    "        else:\n",
    "            url='https://ssbd.riken.jp/neurogt/zip/'+l_strip[n]+str('G')+'.zip'\n",
    "            save_name='dataset/'+str(i)+'/'+l_strip[n]+str('G')+'/'+l_strip[n]+str('G')+'.zip'\n",
    "        \"\"\"\n",
    "        \n",
    "        if i%2 == 0:\n",
    "            url='https://ssbd.riken.jp/neurogt/zip/'+l_strip[i//2-1]+str('L')+'.zip'\n",
    "            save_name='dataset/'+str(i)+'/'+l_strip[i//2-1]+str('L')+'/'+l_strip[i//2-1]+str('L')+'.zip'\n",
    "            \n",
    "        \n",
    "        else:\n",
    "            url='https://ssbd.riken.jp/neurogt/zip/'+l_strip[i//2]+str('G')+'.zip'\n",
    "            save_name='dataset/'+str(i)+'/'+l_strip[i//2]+str('G')+'/'+l_strip[i//2]+str('G')+'.zip'\n",
    "            \n",
    "        urllib.request.urlretrieve(url, save_name)\n",
    "        print(url, save_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d845c108-f8b0-45ae-b60c-d3719a1c4635",
   "metadata": {},
   "source": [
    "## Save the contour of the object as a 3D point cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1384198-058e-4c79-a928-5f4be97eda84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from skimage.io import imread, imshow\n",
    "from skimage import data\n",
    "from skimage.util import img_as_ubyte\n",
    "from skimage.filters.rank import entropy\n",
    "from skimage.morphology import disk\n",
    "from skimage.color import rgb2hsv, rgb2gray, rgb2yuv\n",
    "import concurrent.futures\n",
    "import logging\n",
    "import time\n",
    "from tqdm.notebook import trange\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "640a9620-24dc-464a-bee6-122bc5a938a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Directories\n",
    "def mkdir(path):\n",
    "    if not os.path.exists(path):#ディレクトリがなかったら\n",
    "        os.mkdir(path)#作成したいフォルダ名を作成\n",
    "    \n",
    "#Sorting Numbers\n",
    "def numericalSort(value):\n",
    "    numbers = re.compile(r'(\\d+)')\n",
    "    parts = numbers.split(value)\n",
    "    parts[1::2] = map(int, parts[1::2])\n",
    "    return parts\n",
    "\n",
    "def kmeans(data_in):\n",
    "    data = np.floor(data_in*255)\n",
    "    # ラベルの初期化\n",
    "    labels = np.random.randint(0,2,data.shape[0])\n",
    "\n",
    "    # 終了条件\n",
    "    OPTIMIZE_EPSILON = 1\n",
    "\n",
    "    m_0_old = -np.inf\n",
    "    m_1_old = np.inf\n",
    "\n",
    "    for i in range(1000):\n",
    "    # それぞれの平均の計算\n",
    "        m_0 = data[labels==0].mean()\n",
    "        m_1 = data[labels==1].mean()\n",
    "    # ラベルの再計算\n",
    "        labels[np.abs(data-m_0) < np.abs(data-m_1)] = 0\n",
    "        labels[np.abs(data-m_0) >= np.abs(data-m_1)] = 1\n",
    "    #     終了条件\n",
    "        if np.abs(m_0 - m_0_old) + np.abs(m_1 - m_1_old) < OPTIMIZE_EPSILON:\n",
    "            break\n",
    "        m_0_old = m_0\n",
    "        m_1_old = m_1\n",
    "        \n",
    "    # 初期値によって，クラスが変化するため上界の小さい方を採用\n",
    "    thresh_kmeans = np.minimum(data[labels==0].max(),data[labels==1].max())\n",
    "    \n",
    "    return thresh_kmeans/255\n",
    "\n",
    "\n",
    "def compare(com_path, files, n):\n",
    "    print(com_path, files, n)\n",
    "    img_in = imread(files)\n",
    "    dst = cv2.GaussianBlur(img_in, (5, 5), sigmaX=1)\n",
    "    img_in = rgb2gray(img_in)\n",
    "\n",
    "    dst = rgb2gray(dst)\n",
    "\n",
    "    neiborhood = np.ones((5,5),np.uint8)\n",
    "    \n",
    "    entropy_image_1 = entropy(img_in, neiborhood)\n",
    "    entropy_image_2 = entropy(dst, neiborhood)\n",
    "    \n",
    "    scaled_entropy_1 = entropy_image_1 / entropy_image_1.max() # ガウシアンなし\n",
    "    scaled_entropy_2 = entropy_image_2 / entropy_image_2.max() # ガウシアンあり\n",
    "\n",
    "    a1 = np.floor(scaled_entropy_1*255)\n",
    "    a2 = np.floor(scaled_entropy_2*255)\n",
    "    \n",
    "    a1 = a1.astype('uint8')\n",
    "    a2 = a2.astype('uint8')\n",
    "    \n",
    "    # 画像をヒストグラム化する\n",
    "    image1_hist = cv2.calcHist([a1], [0], None, [256], [0, 256])\n",
    "    image2_hist = cv2.calcHist([a2], [0], None, [256], [0, 256])\n",
    "    \n",
    "    #print(cv2.compareHist(image1_hist, image2_hist, 0))\n",
    "    \n",
    "    np.savetxt(com_path+str(n)+'_compare.txt', [cv2.compareHist(image1_hist, image2_hist, 0)], fmt ='%.6f')\n",
    "\n",
    "#Thresholding by k-means after entropy filter\n",
    "def threshhold(th_path, img_files, com_files, k_num, n):\n",
    "    pin = 0\n",
    "    # データの読み込み\n",
    "    img = cv2.imread(img_files)\n",
    "    #ガウシアンフィルタ\n",
    "    #img = cv2.GaussianBlur(img, (5, 5), sigmaX=1)\n",
    "    # グレースケール変換\n",
    "    shawl_gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    neiborhood = np.ones((5,5),np.uint8)\n",
    "    \n",
    "    scaled_entropy = shawl_gray / shawl_gray.max()\n",
    "    entropy_image = entropy(scaled_entropy, neiborhood)\n",
    "    scaled_entropy = entropy_image / entropy_image.max()\n",
    "    \n",
    "    a_ = np.floor(scaled_entropy*255)\n",
    "    a = a_.astype(np.int64)\n",
    "    data = a.reshape(-1)\n",
    "    \n",
    "    # k-means\n",
    "    # ラベルの初期化\n",
    "    labels = np.random.randint(0,2,data.shape[0])\n",
    "\n",
    "    # 終了条件\n",
    "    OPTIMIZE_EPSILON = 1\n",
    "\n",
    "    m_0_old = -np.inf\n",
    "    m_1_old = np.inf\n",
    "\n",
    "    for i in range(1000):\n",
    "    # それぞれの平均の計算\n",
    "        m_0 = data[labels==0].mean()\n",
    "        m_1 = data[labels==1].mean()\n",
    "    # ラベルの再計算\n",
    "        labels[np.abs(data-m_0) < np.abs(data-m_1)] = 0\n",
    "        labels[np.abs(data-m_0) >= np.abs(data-m_1)] = 1\n",
    "    #     終了条件\n",
    "        if np.abs(m_0 - m_0_old) + np.abs(m_1 - m_1_old) < OPTIMIZE_EPSILON:\n",
    "            break\n",
    "        m_0_old = m_0\n",
    "        m_1_old = m_1\n",
    "\n",
    "    # 初期値によって，クラスが変化するため上界の小さい方を採用\n",
    "    thresh_kmeans = np.minimum(data[labels==0].max(),data[labels==1].max())\n",
    "    \n",
    "    thresh_kmeans = thresh_kmeans/255\n",
    "    \n",
    "    mask = scaled_entropy > thresh_kmeans\n",
    "    \n",
    "    c = mask*255\n",
    "    \n",
    "    #形式変換\n",
    "    c = c.astype('uint8')\n",
    "    \n",
    "    neiborhood = np.ones((3,3),np.uint8)\n",
    "    \n",
    "    print(img_files.split('/')[-1].split('.png')[0])\n",
    "    \n",
    "    if com_files > k_num:\n",
    "        ##ピンボケの場合\n",
    "        #収縮\n",
    "        img_erode = cv2.erode(c,neiborhood,iterations=10)\n",
    "        #膨張\n",
    "        img_dilate = cv2.dilate(img_erode,neiborhood,iterations=10)\n",
    "        cv2.imwrite(th_path+img_files.split('/')[-1].split('.png')[0]+'_k-means.png', img_dilate)\n",
    "\n",
    "    else:\n",
    "        ##気泡の場合\n",
    "        #膨張\n",
    "        img_dilate = cv2.dilate(c,neiborhood,iterations=4)\n",
    "        #収縮\n",
    "        img_erode = cv2.erode(img_dilate,neiborhood,iterations=4)\n",
    "        cv2.imwrite(th_path+img_files.split('/')[-1].split('.png')[0]+'_k-means.png', img_erode)\n",
    "    \n",
    "    \n",
    "#get graphics size \n",
    "def getSize(files,path):\n",
    "    x=[]\n",
    "    y=[]\n",
    "    for n in range(len(files)):\n",
    "        img = cv2.imread(files[n])\n",
    "        h, w = img.shape[:2]\n",
    "        x.append(w)\n",
    "        y.append(h)\n",
    "    \n",
    "    xy = [max(x),max(y)]\n",
    "    np.savetxt(path+'size.txt', xy, fmt ='%.0f')\n",
    "    \n",
    "    #print(\"getSize comp\")\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def padding(pad_path,x,y,files,n):\n",
    "    a_y, a_x = y[n], x[n]\n",
    "    \n",
    "    A_x = (max(x) - a_x)//2\n",
    "    \n",
    "    A_y = (max(y) - a_y)//2\n",
    "    \n",
    "    img_original = cv2.imread(files[n])\n",
    "    \n",
    "    img_pad = cv2.copyMakeBorder(img_original, A_y, A_y, A_x, A_x, cv2.BORDER_CONSTANT, value=(0,0,0))\n",
    "\n",
    "    cv2.imwrite(pad_path+files.split('/')[-1].split('.png')[0]+'_padding.png', img_pad)\n",
    "    \n",
    "    #print(\"padding comp\")\n",
    "    \n",
    "#並列処理用関数\n",
    "def padding_parallel(pad_path,xy,files,k_files,n):\n",
    "    \n",
    "    a_y, a_x = int(xy[1]), int(xy[0])\n",
    "    #print(a_y, a_x)\n",
    "    img_original = cv2.imread(k_files)\n",
    "    #print(files[n])\n",
    "    h, w = img_original.shape[:2]\n",
    "    \n",
    "    A_x = (a_x - w)//2\n",
    "    A_y = (a_y - h)//2\n",
    "    \n",
    "    img_pad = cv2.copyMakeBorder(img_original, A_y, A_y, A_x, A_x, cv2.BORDER_CONSTANT, value=(0,0,0))\n",
    "    \n",
    "    #print(img_pad)\n",
    "    \n",
    "    cv2.imwrite(pad_path+files.split('/')[-1].split('.png')[0]+'_padding.png', img_pad)\n",
    "    \n",
    "    #print(\"padding comp\")\n",
    "    \n",
    "\n",
    "def contour(pad_path,tar_path):\n",
    "\n",
    "    # 入力画像の読み込み\n",
    "    files=sorted(glob.glob(pad_path+'*.png'), key=numericalSort)\n",
    "\n",
    "    for n in range(len(files)):\n",
    "\n",
    "        img_original = cv2.imread(files[n])\n",
    "        \n",
    "        #画像の面積\n",
    "        h, w = img_original.shape[:2]\n",
    "        area = h*w\n",
    "        \n",
    "        # グレースケール変換\n",
    "        gray = cv2.cvtColor(img_original, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        contours, hierarchy = cv2.findContours(gray, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "        approx_contours=[]\n",
    "        for i, cnt in enumerate(contours):\n",
    "            # 輪郭の周囲に比例する精度で輪郭を近似する\n",
    "            arclen = cv2.arcLength(cnt, True)\n",
    "            approx = cv2.approxPolyDP(cnt, arclen*0.00001, True)\n",
    "            approx_contours.append(approx)\n",
    "            \n",
    "            \n",
    "        x_list = []\n",
    "        y_list = []\n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in range(0, len(approx_contours)):\n",
    "            if len(approx_contours[i]) > 0:\n",
    "                #画像に対して面積が小さいものをはじく\n",
    "                if cv2.contourArea(approx_contours[i]) < area//100000:\n",
    "                    #files[0].split('/')[-1]\n",
    "                    continue\n",
    "\n",
    "                buf_np = approx_contours[i].flatten() # numpyの多重配列になっているため、一旦展開する。\n",
    "\n",
    "                for i, elem in enumerate(buf_np):\n",
    "                    if i%2==0:\n",
    "                        x_list.append(elem)\n",
    "                    else:\n",
    "                        y_list.append(elem)\n",
    "\n",
    "\n",
    "        # pandasのSeries型へ一旦変換\n",
    "        x_df = pd.Series(x_list,dtype=\"float64\")\n",
    "        y_df = pd.Series(y_list,dtype=\"float64\")\n",
    "        \n",
    "        files[0].split('/')[-1]\n",
    "        # pandasのDataFrame型へ結合と共に、列名も加えて変換\n",
    "        DF = pd.concat((x_df.rename(r'X'), y_df.rename('Y')), axis=1, sort=False)\n",
    "        DF.to_csv(tar_path+str()+'.csv', encoding=\"utf-8\", index=False)\n",
    "        \n",
    "    print(\"contour comp\")\n",
    "    \n",
    "    \n",
    "def contour_parallel(tar_path,files,pad_files,n):\n",
    "    \n",
    "    img_original = cv2.imread(pad_files)\n",
    "\n",
    "    #画像の面積\n",
    "    h, w = img_original.shape[:2]\n",
    "    area = h*w\n",
    "\n",
    "    # グレースケール変換\n",
    "    gray = cv2.cvtColor(img_original, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    contours, hierarchy = cv2.findContours(gray, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "    approx_contours=[]\n",
    "    for i, cnt in enumerate(contours):\n",
    "        # 輪郭の周囲に比例する精度で輪郭を近似する\n",
    "        arclen = cv2.arcLength(cnt, True)\n",
    "        approx = cv2.approxPolyDP(cnt, arclen*0.00001, True)\n",
    "        approx_contours.append(approx)\n",
    "    \n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    \n",
    "    for i in range(0, len(approx_contours)):\n",
    "        if len(approx_contours[i]) > 0:\n",
    "            #画像に対して面積が小さいものをはじく\n",
    "            if cv2.contourArea(approx_contours[i]) < area//100000:\n",
    "                #files[0].split('/')[-1]\n",
    "                continue\n",
    "\n",
    "            buf_np = approx_contours[i].flatten() # numpyの多重配列になっているため、一旦展開する。\n",
    "\n",
    "            for i, elem in enumerate(buf_np):\n",
    "                if i%2==0:\n",
    "                    x_list.append(elem)\n",
    "                else:\n",
    "                    y_list.append(elem)\n",
    "\n",
    "\n",
    "    # pandasのSeries型へ一旦変換\n",
    "    x_df = pd.Series(x_list,dtype=\"float64\")\n",
    "    y_df = pd.Series(y_list,dtype=\"float64\")\n",
    "\n",
    "    name = files.split('/')[-1].split('.png')[0]\n",
    "    #print(name)\n",
    "    \n",
    "    # pandasのDataFrame型へ結合と共に、列名も加えて変換\n",
    "    DF = pd.concat((x_df.rename(r'X'), y_df.rename('Y')), axis=1, sort=False)\n",
    "    DF.to_csv(tar_path+name+'.csv', encoding=\"utf-8\", index=False)\n",
    "        \n",
    "    #print(\"contour comp\")\n",
    "    \n",
    "def rotate(path, tar_path, rot_path, data_path, name_files):\n",
    "    \n",
    "    # csvの読み込み\n",
    "    files=sorted(glob.glob(tar_path+'*.csv'), key=numericalSort)\n",
    "    \n",
    "    #csvから読み込んでnp.arrayに変換\n",
    "    data = pd.read_csv(files[0]).values.tolist()\n",
    "    _data = np.array(data)\n",
    "\n",
    "    x = _data[:,0]\n",
    "    y = _data[:,1]\n",
    "\n",
    "    #重心\n",
    "    #xg1,yg1 = sum(x)//len(x), sum(y)//len(y)\n",
    "    #中心\n",
    "    xc,yc = (max(x)+min(x))//2, (max(y)+min(y))//2\n",
    "    \n",
    "    num = 0\n",
    "    for n in trange(len(files)):\n",
    "        #print(files[n])\n",
    "        #csvから読み込んでnp.arrayに変換\n",
    "        data = pd.read_csv(files[n]).values.tolist()\n",
    "        _data = np.array(data)\n",
    "        \n",
    "        x1 = []\n",
    "        y1 = []\n",
    "        \n",
    "        x1=_data[:,0]\n",
    "        y1=_data[:,1]\n",
    "        \n",
    "        \n",
    "        #重心\n",
    "        #xg1,yg1 = sum(x1)//len(x1), sum(y1)//len(y1)\n",
    "        #中心\n",
    "        xg1,yg1 = (max(x1)+min(x1))//2, (max(y1)+min(y1))//2\n",
    "        \n",
    "        \n",
    "        x_list = []\n",
    "        y_list = []\n",
    "        z_list = []\n",
    "        rotate_x = []\n",
    "        rotate_y = []\n",
    "        name = []\n",
    "        padding_x = []\n",
    "        padding_y = []\n",
    "        \n",
    "        \n",
    "        # numpyの多重配列になっているため、一旦展開する。    \n",
    "        buf_np = _data.flatten()\n",
    "        \n",
    "        \n",
    "        #z軸\n",
    "        num = files[n].split('/')[-1].split('.png')[0].split('_')[2]\n",
    "        \n",
    "        for i, elem in enumerate(buf_np):\n",
    "            if i%2==0:\n",
    "                x_list.append(elem + (xc-xg1))\n",
    "                rotate_x.append(xc-xg1)\n",
    "            \n",
    "            else:\n",
    "                y_list.append(elem + (yc-yg1))\n",
    "                rotate_y.append(yc-yg1)\n",
    "                z_list.append(int(num))\n",
    "            \n",
    "        \n",
    "        name = [name_files[n].split('/')[-1]] * len(x_list)\n",
    "        img_name = pd.Series(name,dtype=\"string\")\n",
    "        \n",
    "        padding = np.loadtxt(path + \"size.txt\")\n",
    "        padding_x = [padding[0]] * len(x_list)\n",
    "        padding_y = [padding[1]] * len(x_list)\n",
    "        \n",
    "        padding_x_df = pd.Series(padding_x,dtype=\"float64\")\n",
    "        padding_y_df = pd.Series(padding_y,dtype=\"float64\")\n",
    "                   \n",
    "        # pandasのSeries型へ一旦変換\n",
    "        x_df = pd.Series(x_list,dtype=\"float64\")\n",
    "        y_df = pd.Series(y_list,dtype=\"float64\")\n",
    "        z_df = pd.Series(z_list,dtype=\"float64\")\n",
    "        \n",
    "        rotate_x_df = pd.Series(rotate_x,dtype=\"float64\")\n",
    "        rotate_y_df = pd.Series(rotate_y,dtype=\"float64\")\n",
    "        \n",
    "        # pandasのDataFrame型へ結合と共に、列名も加えて変換\n",
    "        DF = pd.concat((x_df.rename(r'X'), y_df.rename('Y'), z_df.rename('Z')), axis=1, sort=False)\n",
    "        DF.to_csv(rot_path+str(n)+'.csv', encoding=\"utf-8\", index=False)\n",
    "        \n",
    "        # 画像名, x,y,z, パディング後のwidth, height, 中心にずらした移動量x,y\n",
    "        DF = pd.concat((img_name.rename(r'Name'), x_df.rename('X'), y_df.rename('Y'), z_df.rename('Z'), \n",
    "                        padding_x_df.rename('Padding_x'), padding_y_df.rename('Padding_y'), rotate_x_df.rename('Center_x'), rotate_y_df.rename('Center_y')), axis=1, sort=False)\n",
    "        DF.to_csv(data_path+str(n)+'.csv', encoding=\"utf-8\", index=False)\n",
    "        \n",
    "        #DF = pd.concat((rotate_x_df.rename(r'Center_x'), rotate_y_df.rename('Center_y')), axis=1, sort=False)\n",
    "        #DF.to_csv(cen_path+str(n)+'_Center.csv', encoding=\"utf-8\", index=False)\n",
    "        \n",
    "    print(\"rotate comp\")\n",
    "\n",
    "    \n",
    "def rotate_parallel(tar_files, rot_path, data_path, xy, files, n):\n",
    "    \n",
    "    #csvから読み込んでnp.arrayに変換\n",
    "    data = pd.read_csv(tar_files).values.tolist()\n",
    "    _data = np.array(data)\n",
    "\n",
    "    x1 = []\n",
    "    y1 = []\n",
    "\n",
    "    x1=_data[:,0]\n",
    "    y1=_data[:,1]\n",
    "    \n",
    "    #重心\n",
    "    #xg1,yg1 = sum(x1)//len(x1), sum(y1)//len(y1)\n",
    "    #中心\n",
    "    xg1,yg1 = (max(x1)+min(x1))//2, (max(y1)+min(y1))//2\n",
    "    \n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    z_list = []\n",
    "    rotate_x = []\n",
    "    rotate_y = []\n",
    "    name = []\n",
    "    padding_x = []\n",
    "    padding_y = []\n",
    "    \n",
    "    # numpyの多重配列になっているため、一旦展開する。    \n",
    "    buf_np = _data.flatten()\n",
    "    \n",
    "    #z軸\n",
    "    #print(files)\n",
    "    num = files.split('/')[-1].split('.png')[0].split('_')[2]\n",
    "    \n",
    "    for i, elem in enumerate(buf_np):\n",
    "        if i%2==0:\n",
    "            \n",
    "            x_list.append(elem + (xy[0]-xg1))\n",
    "            \n",
    "            rotate_x.append(xy[0]-xg1)\n",
    "        else:\n",
    "            \n",
    "            y_list.append(elem + (xy[1]-yg1))\n",
    "            \n",
    "            rotate_y.append(xy[1]-yg1)\n",
    "            \n",
    "            z_list.append(int(num)*20)\n",
    "    \n",
    "    \n",
    "    #名前\n",
    "    name = [files.split('/')[-1]] * len(x_list)    \n",
    "    img_name = pd.Series(name,dtype=\"string\")\n",
    "    \n",
    "    padding_x = [xy[0]] * len(x_list)\n",
    "    padding_y = [xy[1]] * len(x_list)\n",
    "    \n",
    "    # pandasのSeries型へ一旦変換\n",
    "    padding_x_df = pd.Series(padding_x,dtype=\"float64\")\n",
    "    padding_y_df = pd.Series(padding_y,dtype=\"float64\")\n",
    "\n",
    "    x_df = pd.Series(x_list,dtype=\"float64\")\n",
    "    y_df = pd.Series(y_list,dtype=\"float64\")\n",
    "    z_df = pd.Series(z_list,dtype=\"float64\")\n",
    "    \n",
    "    rotate_x_df = pd.Series(rotate_x,dtype=\"float64\")\n",
    "    rotate_y_df = pd.Series(rotate_y,dtype=\"float64\")\n",
    "    \n",
    "    \n",
    "    # pandasのDataFrame型へ結合と共に、列名も加えて変換\n",
    "    DF = pd.concat((x_df.rename(r'X'), y_df.rename('Y'), z_df.rename('Z')), axis=1, sort=False)\n",
    "    DF.to_csv(rot_path+files.split('/')[-1].split('.png')[0]+'_rotate.csv', encoding=\"utf-8\", index=False)\n",
    "    \n",
    "    # 画像名, x,y,z, パディング後のwidth, height, 中心にずらした移動量x,y\n",
    "    DF = pd.concat((img_name.rename(r'Name'), x_df.rename('X'), y_df.rename('Y'), z_df.rename('Z'), \n",
    "                    padding_x_df.rename('Padding_x'), padding_y_df.rename('Padding_y'), rotate_x_df.rename('Center_x'), rotate_y_df.rename('Center_y')), axis=1, sort=False)\n",
    "    DF.to_csv(data_path+files.split('/')[-1].split('.png')[0]+'_bd5.csv', encoding=\"utf-8\", index=False)\n",
    "    \n",
    "    #print(data_path)\n",
    "    \n",
    "    #DF = pd.concat((rotate_x_df.rename(r'Center_x'), rotate_y_df.rename('Center_y')), axis=1, sort=False)\n",
    "    #DF.to_csv(cen_path+str(n)+'_Center.csv', encoding=\"utf-8\", index=False)\n",
    "        \n",
    "    #print(\"rotate comp\")\n",
    "\n",
    "def all_csv(rot_path,path,i):\n",
    "    \n",
    "    All_Files=sorted(glob.glob(rot_path+'*.csv'), key=numericalSort)\n",
    "    \n",
    "    # フォルダ中の全csvをマージ\n",
    "    list = []\n",
    "    for file in All_Files:\n",
    "        list.append(pd.read_csv(file))\n",
    "    df = pd.concat(list, sort=False)\n",
    "    \n",
    "    # csv出力\n",
    "    df.to_csv(path+'datas_'+str(i)+'.csv', encoding='utf_8', index=False)\n",
    "    \n",
    "    print(\"all_csv comp\")\n",
    "    \n",
    "def tag(files,tag_path,n):\n",
    "    img = cv2.imread(files[n])\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    dst = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C,cv2.THRESH_BINARY_INV, 45, np.std(gray))\n",
    "    \n",
    "    points = np.argwhere(dst2 > 0)\n",
    "    \n",
    "    if label == L:\n",
    "        dbscan = DBSCAN(eps=4, min_samples=40)\n",
    "    else:\n",
    "        dbscan = DBSCAN(eps=5, min_samples=70)\n",
    "        \n",
    "    dbscan.fit(points)\n",
    "    \n",
    "    # 密度が閾値以上の点だけを取得\n",
    "    dense_points = points[dbscan.labels_ != -1]\n",
    "    \n",
    "    # 結果を画像に描画\n",
    "    result = np.zeros_like(img)\n",
    "    for point in dense_points:\n",
    "        result[point[0], point[1]] = 255\n",
    "    \n",
    "    cv2.imwrite(tag_path+str(n)+'.png',result)\n",
    "    \n",
    "def img_to_tag(pad_files, tar_files, name_files, tag_csv_path, xy, n):\n",
    "    \n",
    "    img = cv2.imread(pad_files[n], cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    points = np.column_stack(np.where(img != 255))\n",
    "    data = points.flatten()\n",
    "    #x1 = np.array(points[:,0])\n",
    "    #y1 = np.array(points[:,1])\n",
    "    \n",
    "    \n",
    "    xc = np.array(xy[0])\n",
    "    yc = np.array(xy[1])\n",
    "    \n",
    "    if len(points) < 2:\n",
    "        return\n",
    "    \n",
    "    else:\n",
    "        x1 = []\n",
    "        y1 = []\n",
    "        \n",
    "        x_list = []\n",
    "        y_list = []\n",
    "        z_list = []\n",
    "        rotate_x = []\n",
    "        rotate_y = []\n",
    "        name = []\n",
    "        padding_x = []\n",
    "        padding_y = []\n",
    "        \n",
    "        tar_data = pd.read_csv(tar_files[n]).values.tolist()\n",
    "        _data = np.array(tar_data)\n",
    "        \n",
    "        x1=_data[:,0]\n",
    "        y1=_data[:,1]\n",
    "        \n",
    "        #中心\n",
    "        xc1,yc1 = (max(x1)+min(x1))//2, (max(y1)+min(y1))//2\n",
    "        \n",
    "        num = name_files[n].split('/')[-1].split('.png')[0].split('_')[2]\n",
    "        \n",
    "        for m, elem in enumerate(data):\n",
    "            if m%2==0:\n",
    "                x_list.append(elem + (xc-xc1))\n",
    "                rotate_x.append(xc-xc1)\n",
    "            else:\n",
    "                y_list.append(elem + (yc-yc1))\n",
    "                rotate_y.append(yc-yc1)\n",
    "                z_list.append(int(num)*20)\n",
    "                \n",
    "        padding_x = [xy[0]] * len(x_list)\n",
    "        padding_y = [xy[1]] * len(x_list)\n",
    "        \n",
    "        # pandasのSeries型へ一旦変換\n",
    "        padding_x_df = pd.Series(padding_x,dtype=\"float64\")\n",
    "        padding_y_df = pd.Series(padding_y,dtype=\"float64\")\n",
    "        \n",
    "        rotate_x_df = pd.Series(rotate_x,dtype=\"float64\")\n",
    "        rotate_y_df = pd.Series(rotate_y,dtype=\"float64\")\n",
    "        \n",
    "        # pandasのSeries型へ一旦変換\n",
    "        x_df = pd.Series(x_list, dtype=\"float64\")\n",
    "        y_df = pd.Series(y_list, dtype=\"float64\")\n",
    "        z_df = pd.Series(z_list, dtype=\"float64\")\n",
    "        \n",
    "        name = [name_files[n].split('/')[-1]] * len(x_list)\n",
    "        img_name = pd.Series(name,dtype=\"string\")\n",
    "        \n",
    "        #DF = pd.concat((x_df.rename(r'X'), y_df.rename('Y'), z_df.rename('Z')), axis=1, sort=False)\n",
    "        DF = pd.concat((img_name.rename(r'Name'), x_df.rename('X'), y_df.rename('Y'), z_df.rename('Z'), \n",
    "                        padding_x_df.rename('Padding_x'), padding_y_df.rename('Padding_y'), rotate_x_df.rename('Center_x'), rotate_y_df.rename('Center_y')), axis=1, sort=False)\n",
    "        #z = np.array(z)\n",
    "        #xyz = np.concatenate((points, z), axis = 1, dtype='int')\n",
    "        #xyz = np.concatenate((x_c,y_c, z), axis = 1, dtype='int')\n",
    "        #DF = pd.DataFrame(xyz,columns =['X','Y','Z'])\n",
    "        # csv出力\n",
    "        DF.to_csv(tag_csv_path+str(name_files[n].split('/')[-1])+'.csv', encoding='utf_8', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a71bc12-ae78-4964-823f-5d60904ae48a",
   "metadata": {},
   "source": [
    "## Create contour data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d67c72-61f6-4d33-93ff-0c9dbb858116",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.020563125610351562,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 5,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea68a41ed1854011bf6601449479f67b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.016310691833496094,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec6aa54bf1dc48fdbf1f74f183b44f8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0063211917877197266,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 27,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "389a84526f944bd999e97acc92786861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/67/compare_v1/dataset/67/compare_v1/dataset/67/compare_v1/dataset/67/compare_v1/ dataset/67/compare_v1/  dataset/67/G2AN_TM11.5_G/G2AN_TM11.5_0003_G004_1.pngdataset/67/G2AN_TM11.5_G/G2AN_TM11.5_0013_G004_2.png 1 \n",
      " dataset/67/G2AN_TM11.5_G/G2AN_TM11.5_0000_G001_1.png4 0\n",
      " \n",
      " dataset/67/G2AN_TM11.5_G/G2AN_TM11.5_0016_G007_2.pngdataset/67/G2AN_TM11.5_G/G2AN_TM11.5_0010_G001_2.png5 3\n",
      "dataset/67/compare_v1/ dataset/67/G2AN_TM11.5_G/G2AN_TM11.5_0006_G007_1.png 2\n",
      "\n",
      "dataset/67/compare_v1/dataset/67/compare_v1/dataset/67/compare_v1/dataset/67/compare_v1/dataset/67/compare_v1/dataset/67/compare_v1/   dataset/67/G2AN_TM11.5_G/G2AN_TM11.5_0020_G001_3.png  dataset/67/G2AN_TM11.5_G/G2AN_TM11.5_0036_G007_4.pngdataset/67/G2AN_TM11.5_G/G2AN_TM11.5_0030_G001_4.png dataset/67/G2AN_TM11.5_G/G2AN_TM11.5_0023_G004_3.png  11 \n",
      "7dataset/67/G2AN_TM11.5_G/G2AN_TM11.5_0026_G007_3.png 8\n",
      " 9\n",
      "\n",
      "6\n",
      "dataset/67/G2AN_TM11.5_G/G2AN_TM11.5_0033_G004_4.png 10\n",
      "dataset/67/compare_v1/dataset/67/compare_v1/dataset/67/compare_v1/dataset/67/compare_v1/dataset/67/compare_v1/dataset/67/compare_v1/    dataset/67/G2AN_TM11.5_G/G2AN_TM11.5_0040_G001_5.png dataset/67/G2AN_TM11.5_G/G2AN_TM11.5_0050_G001_6.png dataset/67/G2AN_TM11.5_G/G2AN_TM11.5_0046_G007_5.png dataset/67/G2AN_TM11.5_G/G2AN_TM11.5_0053_G004_6.png14  1216\n",
      "dataset/67/G2AN_TM11.5_G/G2AN_TM11.5_0043_G004_5.png\n",
      "\n",
      " 13\n",
      " dataset/67/G2AN_TM11.5_G/G2AN_TM11.5_0056_G007_6.png15\n",
      " 17\n",
      "dataset/67/compare_v1/dataset/67/compare_v1/dataset/67/compare_v1/dataset/67/compare_v1/dataset/67/compare_v1/dataset/67/compare_v1/     dataset/67/G2AN_TM11.5_G/G2AN_TM11.5_0073_G004_8.pngdataset/67/G2AN_TM11.5_G/G2AN_TM11.5_0070_G001_8.pngdataset/67/G2AN_TM11.5_G/G2AN_TM11.5_0066_G007_7.png   20dataset/67/G2AN_TM11.5_G/G2AN_TM11.5_0076_G007_8.pngdataset/67/G2AN_TM11.5_G/G2AN_TM11.5_0063_G004_7.png22\n",
      " 23\n",
      "\n",
      "dataset/67/G2AN_TM11.5_G/G2AN_TM11.5_0060_G001_7.png  \n",
      "2118\n",
      " 19\n",
      "dataset/67/compare_v1/dataset/67/compare_v1/dataset/67/compare_v1/dataset/67/compare_v1/dataset/67/compare_v1/ dataset/67/compare_v1/dataset/67/G2AN_TM11.5_G/G2AN_TM11.5_0086_G007_9.png    dataset/67/G2AN_TM11.5_G/G2AN_TM11.5_0080_G001_9.pngdataset/67/G2AN_TM11.5_G/G2AN_TM11.5_0093_G004_10.png  dataset/67/G2AN_TM11.5_G/G2AN_TM11.5_0083_G004_9.png dataset/67/G2AN_TM11.5_G/G2AN_TM11.5_0096_G007_10.png2624  \n",
      "\n",
      "2529\n",
      "\n",
      "dataset/67/G2AN_TM11.5_G/G2AN_TM11.5_0090_G001_10.png  27\n",
      "28\n",
      "dataset/67/compare_v1/dataset/67/compare_v1/dataset/67/compare_v1/dataset/67/compare_v1/dataset/67/compare_v1/dataset/67/compare_v1/   dataset/67/G2AN_TM11.5_G/G2AN_TM11.5_0113_G004_12.png "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    # 1 if all imaging datasets are download from the internet,\n",
    "    # 0 if some imaging datasets have already been download locally, \n",
    "    \n",
    "    DOWNLOAD_ALL = 0\n",
    "    \n",
    "    if DOWNLOAD_ALL == 0:\n",
    "        dataset_start = 67\n",
    "        dataset_end = 69\n",
    "    \n",
    "    elif DOWNLOAD_ALL == 1:\n",
    "        dataset_start = 0\n",
    "        dataset_end = 85\n",
    "    \n",
    "    for t in trange(0,5):\n",
    "        print(t)\n",
    "        for i in trange(dataset_start, dataset_end):\n",
    "            \n",
    "            path = 'dataset/'+str(i)+'/'\n",
    "            \n",
    "            files=sorted(glob.glob(path+'[D]*/*.png'), key=numericalSort)\n",
    "            if len(files) == 0:\n",
    "                files=sorted(glob.glob(path+'[G]*/*.png'), key=numericalSort)\n",
    "            print(len(files))\n",
    "            \n",
    "            version = '_v1'\n",
    "            \n",
    "            #The number of cores of the cpu to use\n",
    "            process = 6\n",
    "            #process = 1\n",
    "            \n",
    "            com_path = path + 'compare'+version+'/'\n",
    "            mkdir(com_path)\n",
    "            \n",
    "            th_path = path + 'k-means_threshold'+version+'/'\n",
    "            mkdir(th_path)\n",
    "            \n",
    "            pad_path = path + 'img_padding'+version+'/'\n",
    "            mkdir(pad_path)\n",
    "            \n",
    "            tar_path = path + 'target_contour'+version+'/'\n",
    "            mkdir(tar_path)\n",
    "            \n",
    "            rot_path = path + 'rotate_filter'+version+'/'\n",
    "            mkdir(rot_path)\n",
    "            \n",
    "            data_path = path + 'ImageData'+version+'/'\n",
    "            mkdir(data_path)\n",
    "            \n",
    "            a = len(files) - len(files) % process\n",
    "            \n",
    "            #nomal task\n",
    "            if t == 0:\n",
    "                for n in trange(len(files)//process):\n",
    "                    with concurrent.futures.ProcessPoolExecutor(max_workers=process) as executor:\n",
    "                        for m in range(process):\n",
    "                            executor.submit(compare, com_path, files[n*process+m], n*process+m)\n",
    "                            \n",
    "                with concurrent.futures.ProcessPoolExecutor(max_workers=process) as executor:\n",
    "                    for a_i in range(a,len(files)):\n",
    "                        executor.submit(compare, com_path, files[a_i], a_i)\n",
    "                        \n",
    "            #######################################################\n",
    "            \n",
    "            if t == 1:\n",
    "                #com_path = path + 'compare_v1/'\n",
    "                com=sorted(glob.glob(com_path+'*.txt'), key=numericalSort)\n",
    "                com_files=[]\n",
    "                \n",
    "                for d in com:\n",
    "                    com_files.append(np.loadtxt(d))\n",
    "                    \n",
    "                k_num = kmeans(com_files)\n",
    "                \n",
    "                for n in trange(len(files)//process):\n",
    "                    with concurrent.futures.ProcessPoolExecutor(max_workers=process) as executor:\n",
    "                        for m in range(process):\n",
    "                            executor.submit(threshhold, th_path, files[n*process+m], com_files[n*process+m], k_num, n*process+m)\n",
    "                            \n",
    "                with concurrent.futures.ProcessPoolExecutor(max_workers=process) as executor:\n",
    "                    for a_i in range(a,len(files)):\n",
    "                        executor.submit(threshhold, th_path, files[a_i], com_files[a_i], k_num, a_i)\n",
    "            \n",
    "            #######################################################\n",
    "            \n",
    "            if t == 2:\n",
    "                getSize(files,path)\n",
    "                size_path = path+\"/size.txt\"\n",
    "                \n",
    "                xy = np.loadtxt(size_path)\n",
    "                \n",
    "                k_files=sorted(glob.glob(th_path+'*.png'), key=numericalSort)\n",
    "                \n",
    "                for n in trange(len(k_files)//process):\n",
    "                    with concurrent.futures.ProcessPoolExecutor(max_workers=process) as executor:\n",
    "                        for m in range(process):\n",
    "                            executor.submit(padding_parallel, pad_path, xy, files[n*process+m], k_files[n*process+m], n*process+m)\n",
    "                            \n",
    "                with concurrent.futures.ProcessPoolExecutor(max_workers=process) as executor:\n",
    "                    for a_i in trange(a,len(files)):\n",
    "                        executor.submit(padding_parallel, pad_path, xy, files[a_i], k_files[a_i], a_i)\n",
    "            \n",
    "            #######################################################\n",
    "            \n",
    "            if t == 3:\n",
    "                \n",
    "                pad_files=sorted(glob.glob(pad_path+'*.png'), key=numericalSort)\n",
    "                for n in trange(len(files)//process):\n",
    "                    with concurrent.futures.ProcessPoolExecutor(max_workers=process) as executor:\n",
    "                        for m in range(process):\n",
    "                            executor.submit(contour_parallel, tar_path, files[n*process+m], pad_files[n*process+m], n*process+m)\n",
    "                            \n",
    "                with concurrent.futures.ProcessPoolExecutor(max_workers=process) as executor:\n",
    "                    for a_i in trange(a,len(files)):\n",
    "                        executor.submit(contour_parallel, tar_path, files[a_i], pad_files[a_i], a_i)\n",
    "                        \n",
    "            #######################################################\n",
    "            \n",
    "            if t == 4:\n",
    "                size_path = path+\"/size.txt\"\n",
    "                xy = np.loadtxt(size_path)\n",
    "                \n",
    "                tar_files=sorted(glob.glob(tar_path+'*.csv'), key=numericalSort)\n",
    "                \n",
    "                #csvから読み込んでnp.arrayに変換\n",
    "                data = pd.read_csv(tar_files[0]).values.tolist()\n",
    "                _data = np.array(data)\n",
    "                \n",
    "                x = _data[:,0]\n",
    "                y = _data[:,1]\n",
    "                \n",
    "                #中心\n",
    "                xy = [(max(x)+min(x))//2, (max(y)+min(y))//2]\n",
    "                \n",
    "                \n",
    "                for n in trange(len(files)//process):\n",
    "                    with concurrent.futures.ProcessPoolExecutor(max_workers=process) as executor:\n",
    "                        for m in range(process):\n",
    "                            executor.submit(rotate_parallel, tar_files[n*process+m], rot_path, data_path, xy, files[n*process+m], n*process+m)\n",
    "                            \n",
    "                with concurrent.futures.ProcessPoolExecutor(max_workers=process) as executor:\n",
    "                    for a_i in trange(a,len(files)):\n",
    "                        executor.submit(rotate_parallel, tar_files[a_i], rot_path, data_path, xy, files[a_i], a_i)\n",
    "                        \n",
    "            #######################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ad776b-7d30-44a9-ac98-e1a43f20a78d",
   "metadata": {},
   "source": [
    "## Creation of data for tagging area portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2332e7bc-d212-482f-8498-b7c7392cc260",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    for t in trange(3):\n",
    "        for i in trange(1,85):\n",
    "\n",
    "            path = str(i)+'/'\n",
    "\n",
    "            files=sorted(glob.glob(path+'[D]*/*.png'), key=numericalSort)\n",
    "\n",
    "            if len(files) == 0:\n",
    "                files=sorted(glob.glob(path+'[G]*/*.png'), key=numericalSort)\n",
    "                \n",
    "            \n",
    "            tag_path = path + 'tag_img'+version+'/'\n",
    "            mkdir(tag_path)\n",
    "            pad_path = path + 'tag_img_padding'+version+'/'\n",
    "            mkdir(pad_path)\n",
    "            csv_path = path + 'tag_csv'+version+'/'\n",
    "            mkdir(csv_path)\n",
    "            \n",
    "            \n",
    "            #######################################################\n",
    "            \n",
    "            if t == 0:\n",
    "                getSize(files,path)\n",
    "                size_path = path+\"/size.txt\"\n",
    "\n",
    "                xy = np.loadtxt(size_path)\n",
    "\n",
    "                files=sorted(glob.glob(path+'k-means_threshhold/*/[0-9]*.png'), key=numericalSort)\n",
    "\n",
    "                for n in trange(len(files)//process):\n",
    "                    with concurrent.futures.ProcessPoolExecutor(max_workers=process) as executor:\n",
    "                        for m in range(process):\n",
    "                            executor.submit(tag, files, tag_path, n*process+m)\n",
    "\n",
    "                with concurrent.futures.ProcessPoolExecutor(max_workers=6) as executor:\n",
    "                    for a_i in trange(a,len(files)):\n",
    "                        executor.submit(tag, files, tag_path, a_i)\n",
    "            \n",
    "            #######################################################\n",
    "            \n",
    "            #######################################################\n",
    "            \n",
    "            if t == 1:\n",
    "                getSize(files,path)\n",
    "                size_path = path+\"/size.txt\"\n",
    "\n",
    "                xy = np.loadtxt(size_path)\n",
    "\n",
    "                files=sorted(glob.glob(path+'k-means_threshhold/*/[0-9]*.png'), key=numericalSort)\n",
    "\n",
    "                for n in trange(len(files)//process):\n",
    "                    with concurrent.futures.ProcessPoolExecutor(max_workers=process) as executor:\n",
    "                        for m in range(process):\n",
    "                            executor.submit(padding_parallel, pad_path,xy,files,n*process+m)\n",
    "\n",
    "                with concurrent.futures.ProcessPoolExecutor(max_workers=6) as executor:\n",
    "                    for a_i in trange(a,len(files)):\n",
    "                        executor.submit(padding_parallel, pad_path,xy,files,a_i)\n",
    "            \n",
    "            #######################################################\n",
    "            \n",
    "            #######################################################\n",
    "            \n",
    "            if t == 2:\n",
    "                tar_path = path + 'target_contour'+version+'/'\n",
    "                tar_files = sorted(glob.glob(tar_path+'*.csv'), key=numericalSort)\n",
    "                pad_files=sorted(glob.glob(pad_path+'*.png'), key=numericalSort)\n",
    "                \n",
    "                size_path = path+\"/size.txt\"\n",
    "                xy = np.loadtxt(size_path)\n",
    "\n",
    "                for n in trange(len(name_files)//process):\n",
    "                    with concurrent.futures.ProcessPoolExecutor(max_workers=process) as executor:\n",
    "                        for m in range(process):\n",
    "                            executor.submit(img_to_tag, pad_files, tar_files, files, tag_csv_path, xy,  n*process+m)\n",
    "\n",
    "                with concurrent.futures.ProcessPoolExecutor(max_workers=process) as executor:\n",
    "                    for a_i in range(a,len(name_files)):\n",
    "                        executor.submit(img_to_tag, pad_files, tar_files, files, tag_csv_path, xy, a_i)\n",
    "            \n",
    "            #######################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
